# üîí Setup de IA Privada para An√°lisis de Notion

## üéØ Objetivo

Configurar una soluci√≥n de IA completamente privada que procese documentaci√≥n de Notion sin exponer datos a APIs p√∫blicas.

## üèóÔ∏è Opciones de Implementaci√≥n

### Opci√≥n 1: Ollama (Recomendado) ‚≠ê

**Ventajas:**
- ‚úÖ Setup muy simple
- ‚úÖ Modelos open-source gratuitos
- ‚úÖ 100% local, datos nunca salen
- ‚úÖ Buena calidad con modelos modernos
- ‚úÖ No requiere GPU (aunque ayuda)

**Instalaci√≥n:**

```bash
# Windows (usando WSL o Docker)
# Opci√≥n A: Docker
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama

# Opci√≥n B: Instalador nativo
# Descargar de: https://ollama.ai/download

# Descargar modelo (recomendado para an√°lisis)
ollama pull llama2:13b
# o m√°s r√°pido pero menos preciso:
ollama pull mistral:7b
# o mejor para c√≥digo/documentaci√≥n t√©cnica:
ollama pull codellama:13b

# Verificar
ollama list
ollama run llama2:13b "Hello, test"
```

**Configuraci√≥n en el proyecto:**

```javascript
// src/config/aiConfig.js
export const AI_CONFIG = {
  provider: 'ollama',
  baseUrl: process.env.OLLAMA_API_URL || 'http://localhost:11434',
  model: process.env.OLLAMA_MODEL || 'llama2:13b',
  timeout: 30000, // 30 segundos
  temperature: 0.3, // M√°s determinista para an√°lisis
  maxTokens: 2000
};
```

### Opci√≥n 2: Supabase Edge Function

**Ventajas:**
- ‚úÖ Integrado con Supabase
- ‚úÖ Datos nunca salen de Supabase
- ‚úÖ Puede usar Ollama interno o modelo local
- ‚úÖ F√°cil deployment

**Implementaci√≥n:**

```typescript
// supabase/functions/analyze-notion/index.ts
import { serve } from "https://deno.land/std@0.168.0/http/server.ts"
import { createClient } from 'https://esm.sh/@supabase/supabase-js@2'

// Llamar a Ollama interno o procesar localmente
serve(async (req) => {
  const { content, initiativeName } = await req.json()
  
  // Llamar a Ollama (debe estar accesible desde Supabase)
  const ollamaResponse = await fetch('http://ollama:11434/api/generate', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      model: 'llama2:13b',
      prompt: `Analiza esta documentaci√≥n...\n\n${content}`,
      stream: false
    })
  })
  
  // Procesar respuesta y guardar en Supabase
  // ...
})
```

### Opci√≥n 3: Servidor Privado Interno

**Ventajas:**
- ‚úÖ Control total
- ‚úÖ Puede usar GPU dedicado
- ‚úÖ M√°xima seguridad
- ‚úÖ Escalable

**Setup:**

```bash
# Servidor con GPU (opcional pero recomendado)
# Instalar CUDA, PyTorch, etc.
# Setup de API REST con FastAPI o similar
```

## üîß Integraci√≥n en el Proyecto

### 1. Crear Servicio de IA Privada

```javascript
// src/services/privateAiService.js
import { AI_CONFIG } from '../config/aiConfig.js';

export class PrivateAIService {
  constructor() {
    this.baseUrl = AI_CONFIG.baseUrl;
    this.model = AI_CONFIG.model;
  }

  async analyze(content, prompt) {
    try {
      // Si es Ollama
      if (AI_CONFIG.provider === 'ollama') {
        return await this.analyzeWithOllama(content, prompt);
      }
      
      // Si es Edge Function
      if (AI_CONFIG.provider === 'edge-function') {
        return await this.analyzeWithEdgeFunction(content, prompt);
      }
      
      throw new Error('AI provider not configured');
    } catch (error) {
      console.error('[AI] Error analyzing:', error);
      throw error;
    }
  }

  async analyzeWithOllama(content, prompt) {
    const fullPrompt = `${prompt}\n\nDocumentaci√≥n:\n${content}\n\nResponde en formato JSON v√°lido.`;
    
    const response = await fetch(`${this.baseUrl}/api/generate`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: this.model,
        prompt: fullPrompt,
        stream: false,
        options: {
          temperature: 0.3,
          num_predict: 2000
        }
      })
    });

    if (!response.ok) {
      throw new Error(`Ollama API error: ${response.statusText}`);
    }

    const data = await response.json();
    
    // Parsear respuesta JSON
    try {
      return JSON.parse(data.response);
    } catch (e) {
      // Si no es JSON v√°lido, intentar extraerlo
      const jsonMatch = data.response.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        return JSON.parse(jsonMatch[0]);
      }
      throw new Error('Invalid JSON response from AI');
    }
  }

  async analyzeWithEdgeFunction(content, prompt) {
    const { createClient } = await import('@supabase/supabase-js');
    const supabase = createClient(
      process.env.VITE_SUPABASE_URL,
      process.env.VITE_SUPABASE_ANON_KEY
    );

    const { data, error } = await supabase.functions.invoke('analyze-notion', {
      body: { content, prompt, initiativeName }
    });

    if (error) throw error;
    return data;
  }
}

export const privateAIService = new PrivateAIService();
```

### 2. Configuraci√≥n de Variables de Entorno

```env
# .env.local
# Opci√≥n Ollama
OLLAMA_API_URL=http://localhost:11434
OLLAMA_MODEL=llama2:13b

# Opci√≥n Edge Function
AI_EDGE_FUNCTION_URL=https://[project].supabase.co/functions/v1/analyze-notion

# Configuraci√≥n general
AI_PROVIDER=ollama  # 'ollama', 'edge-function', o 'private-server'
```

## üß™ Testing

```javascript
// scripts/test-private-ai.js
import { privateAIService } from '../src/services/privateAiService.js';

const testContent = `
# Strata Public API

## Estado: En Progreso
## Completaci√≥n: 30%

### Tareas
- [x] Dise√±o de API (5 SP)
- [x] Setup inicial (3 SP)
- [ ] Implementaci√≥n endpoints (20 SP)
- [ ] Documentaci√≥n (8 SP)
- [ ] Testing (4 SP)

### Bloqueos
- Esperando aprobaci√≥n de dise√±o de API

### Dependencias
- DataLake debe estar completo
`;

const prompt = `
Analiza esta documentaci√≥n y extrae:
1. Estado (in_progress, blocked, done, planned)
2. Porcentaje de completaci√≥n (0-100)
3. Tareas completadas vs totales
4. Story points completados vs totales
5. Bloqueos mencionados
6. Dependencias

Responde en JSON:
{
  "status": "...",
  "completion": 0-100,
  "tasksCompleted": 0,
  "tasksTotal": 0,
  "storyPointsDone": 0,
  "storyPointsTotal": 0,
  "blockers": [],
  "dependencies": []
}
`;

async function test() {
  try {
    console.log('üß™ Testing private AI...');
    const result = await privateAIService.analyze(testContent, prompt);
    console.log('‚úÖ Result:', JSON.stringify(result, null, 2));
  } catch (error) {
    console.error('‚ùå Error:', error);
  }
}

test();
```

## üîí Seguridad y Privacidad

### Checklist de Privacidad

- ‚úÖ **Datos nunca salen de infraestructura privada**
- ‚úÖ **No hay llamadas a APIs p√∫blicas de IA**
- ‚úÖ **Modelos ejecutados localmente o en servidor privado**
- ‚úÖ **Comunicaci√≥n interna solamente**
- ‚úÖ **Sin logging de datos sensibles en servicios externos**

### Recomendaciones

1. **Red Privada**: Si usas Ollama, aseg√∫rate que est√© en red privada
2. **Autenticaci√≥n**: Proteger API de IA con autenticaci√≥n
3. **Encriptaci√≥n**: Usar HTTPS para comunicaci√≥n interna
4. **Auditor√≠a**: Logging de accesos sin contenido sensible
5. **Backup**: Backup de an√°lisis sin exponer datos

## üìä Comparaci√≥n de Opciones

| Caracter√≠stica | Ollama | Edge Function | Servidor Privado |
|---------------|--------|---------------|------------------|
| Setup | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê F√°cil | ‚≠ê‚≠ê‚≠ê Medio | ‚≠ê‚≠ê Complejo |
| Privacidad | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê 100% | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê 100% | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê 100% |
| Rendimiento | ‚≠ê‚≠ê‚≠ê Bueno | ‚≠ê‚≠ê‚≠ê‚≠ê Muy bueno | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excelente |
| Costo | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Gratis | ‚≠ê‚≠ê‚≠ê‚≠ê Bajo | ‚≠ê‚≠ê‚≠ê Medio |
| Escalabilidad | ‚≠ê‚≠ê‚≠ê Limitada | ‚≠ê‚≠ê‚≠ê‚≠ê Buena | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excelente |

## üöÄ Pr√≥ximos Pasos

1. **Elegir opci√≥n** (recomendado: Ollama para empezar)
2. **Instalar y configurar**
3. **Probar con datos de ejemplo**
4. **Integrar con Notion service**
5. **Conectar con Supabase**
